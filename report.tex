\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math environments (bmatrix, etc.)
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc}
\graphicspath{ {./images/} }


\title{Design and Implementation of a Monocular Measurement System on Android using Dynamic Camera Intrinsics}


\author{
 Trung Duc Bui \\
  MSE28HN\\
  FPT School of Business & Technology\\
  Hanoi, Vietnam \\
  \texttt{trungbuiducbuiduc@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
    Estimating real-world object dimensions from smartphone images is a fundamental problem in computer vision, with applications in e-commerce, logistics, and interior design. Commercial solutions typically rely on augmented reality (AR) frameworks such as ARCore or ARKit, which provide metric scale through SLAM-based 3D reconstruction. However, these frameworks act as ``black boxes'' that obscure the underlying geometric principles, impose operational constraints, and---critically---are not available on all devices, as ARCore certification covers only a subset of Android smartphones. This paper presents the design and implementation of a \textit{traditional, AR-free measurement pipeline} for Android that operates independently of proprietary AR SDKs, enabling metric measurement on any device with Camera2 API support. The proposed system implements a \textit{Dynamic Intrinsics} mechanism, recalculating the camera matrix $K$ per-frame based on \texttt{SCALER\_CROP\_REGION} metadata to maintain accuracy across continuous digital zoom levels. By fusing optical data with IMU-derived orientation, the system supports classical measurement models including ground-plane homography, planar object rectification, and single-view metrology. The application features a dual-mode design: a simplified \textit{User Mode} for practical measurement tasks, and a dedicated \textit{Researcher Mode} that exposes internal parameters and enables experimental analysis. Systematic experiments on a Xiaomi Poco X6 Pro demonstrate that classical computer vision techniques, when properly controlled for intrinsic parameter variations, can achieve practical accuracy while offering full transparency, interpretability, and broad device compatibility.
\end{abstract}


% keywords can be removed
\keywords{Single-View Metrology \and Dynamic Camera Intrinsics \and Android Camera2 API \and Digital Zoom Compensation \and Pinhole Camera Model \and Ground-Plane Homography \and Mobile Measurement}


\section{Introduction}

Estimating real-world object dimensions from images captured by consumer smartphones is a fundamental problem in computer vision, with practical applications spanning e-commerce (product sizing), logistics (package dimensioning), interior design (room measurement), and augmented reality. Over the past decade, mobile devices have witnessed remarkable advances in optical hardware, equipping modern smartphones with high-resolution sensors, optical image stabilization (OIS), and low-level hardware access via APIs such as Android's Camera2. While this evolution positions smartphones as potential precision instruments for metrology, realizing a mathematically transparent measurement tool remains a significant challenge.

Modern smartphones possess powerful cameras and sensors capable of metric measurement, yet most practical solutions rely on proprietary frameworks that obscure the underlying geometric principles. These frameworks, while convenient, act as "black boxes" that hide implementation details, impose operational constraints (requiring camera motion for initialization, texture-rich environments), and are often unavailable on many devices. For researchers, educators, and learners seeking to understand, validate, or extend measurement algorithms, such opacity and limited availability represent significant barriers.

A particularly underexplored challenge within this domain is the \textit{dynamic nature of camera intrinsics during digital zoom operations}. Unlike optical zoom which physically adjusts lens elements, digital zoom fundamentally alters the effective camera matrix through sensor cropping and resampling. The intrinsic parameters---focal length and principal point---change with each zoom level, yet standard measurement applications often assume a fixed camera matrix, leading to systematic errors that compound at higher magnification. This problem is exacerbated by the fact that few existing systems leverage the rich per-frame metadata available through the Camera2 API, particularly \texttt{SCALER\_CROP\_REGION}.

This paper addresses these challenges by designing a \textit{fully transparent measurement pipeline} that applies classical computer vision techniques to the smartphone measurement problem. Drawing on foundational methods in camera calibration~\cite{zhang2000flexible}, projective geometry~\cite{hartley2003multiple}, and single-view metrology~\cite{criminisi2000single}, the proposed system demonstrates that transparent, interpretable approaches can achieve practical accuracy while enabling deep understanding and experimentation.

The system implements several key technical contributions. First, a \textit{Dynamic Intrinsics} mechanism continuously recalculates the camera matrix $K$ based on per-frame \texttt{SCALER\_CROP\_REGION} metadata from the Android Camera2 API. This addresses a critical limitation of fixed-calibration approaches: when users apply digital zoom, the effective crop region changes, and the intrinsic parameters must be recomputed to maintain accuracy. Second, the pipeline integrates multiple classical measurement modules: (1)~ground-plane homography for objects on flat surfaces, leveraging known camera height and IMU orientation; (2)~perspective rectification for planar objects using four-point homography; and (3)~single-view metrology using vanishing point detection for vertical structures. Third, the image processing pipeline applies classical techniques including Canny edge detection, Harris corner refinement, subpixel localization, and RANSAC outlier rejection to improve robustness.

Uniquely, the application features a dual-mode design: a simplified \textit{User Mode} for end-users requiring quick, practical measurements, and a \textit{Researcher Mode} that exposes all internal parameters, supports toggling of processing steps (lens undistortion, edge-based point snapping, perspective rectification), displays debugging overlays (principal point, grid, detected edges, IMU orientation), and enables export of detailed measurement logs (JSON/CSV format) for offline analysis. This design serves both practical measurement needs and educational/research objectives.

The proposed pipeline is validated through systematic experiments on a Xiaomi Poco X6 Pro, a mid-range Android device with full Camera2 API support. Experimental results demonstrate that the Dynamic Intrinsics mechanism is essential for maintaining accuracy across zoom levels, and that classical geometric techniques---when properly implemented with careful attention to calibration, distortion correction, and subpixel refinement---can achieve practical measurement accuracy while providing full transparency, interpretability, and reproducibility.

The main contributions of this work are:
\begin{enumerate}
    \item A \textbf{Dynamic Intrinsics mechanism} that recalculates the camera matrix $K_{out}$ per-frame based on \texttt{SCALER\allowbreak\_CROP\allowbreak\_REGION} metadata, maintaining measurement accuracy across continuous zoom levels where fixed-calibration approaches fail.

    \item Integration of \textbf{multiple classical measurement modules}: ground-plane homography with IMU fusion, perspective rectification for planar objects, and single-view metrology with vanishing points.

    \item A \textbf{comprehensive image processing pipeline} applying edge detection (Canny), corner refinement (Harris, subpixel), contour analysis, and RANSAC outlier rejection to improve measurement robustness.

    \item A \textbf{dual-mode application design} with User Mode for practical measurements and Researcher Mode exposing all parameters, processing toggles, debug overlays, and detailed logging for educational and research purposes.

    \item \textbf{Systematic experimental validation} demonstrating the importance of Dynamic Intrinsics across zoom levels and quantifying the impact of individual processing options (undistortion, edge snapping).
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec:literature} reviews related work on camera calibration, single-view metrology, and existing measurement approaches. Section~\ref{sec:system} describes the proposed system architecture and measurement modules. Section~\ref{sec:experiment} presents the experimental setup and methodology. Section~\ref{sec:results} reports results and discussion. Section~\ref{sec:conclusion} concludes the paper, and Section~\ref{sec:future} outlines directions for future work.

\section{Literature Review}
\label{sec:literature}

This section establishes the theoretical foundations of the proposed system, covering the geometric modeling of cameras, the specific challenges of digital image formation on mobile devices, and existing techniques for extracting metric dimensions from 2D images.

\subsection{Pinhole Camera Model and Calibration}

\subsubsection{The Pinhole Model}

The pinhole camera model serves as the fundamental geometric abstraction for image formation in computer vision~\cite{hartley2003multiple}. This idealized model assumes that all light rays pass through a single point (the optical center) before striking the image plane. Under this model, a 3D world point $\mathbf{X}_w = (X, Y, Z)^T$ is projected onto the 2D image plane at pixel coordinates $(u, v)$ through a linear transformation in homogeneous coordinates:

\begin{equation}
    s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = K [R \mid t] \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}
    \label{eq:pinhole}
\end{equation}

where $s$ is an arbitrary scale factor arising from the use of homogeneous coordinates. The $3 \times 4$ projection matrix $P = K[R \mid t]$ decomposes into two components: the \textit{extrinsic parameters} $[R \mid t]$ describing the camera's pose (rotation $R \in SO(3)$ and translation $t \in \mathbb{R}^3$) in world coordinates, and the \textit{intrinsic matrix} $K$ encoding the internal camera geometry:

\begin{equation}
    K = \begin{bmatrix}
        f_x & s   & c_x \\
        0   & f_y & c_y \\
        0   & 0   & 1
    \end{bmatrix}
    \label{eq:intrinsic}
\end{equation}

Here, $f_x$ and $f_y$ represent the focal length expressed in pixel units along the horizontal and vertical axes respectively, $(c_x, c_y)$ denotes the principal point (the intersection of the optical axis with the image plane), and $s$ is the skew coefficient (typically zero for modern sensors with rectangular pixels).

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Pinhole camera geometry diagram
% Content: 3D illustration showing world coordinate system, camera coordinate system,
% optical center, image plane, and the projection of a 3D point onto the image.
% Caption: "The pinhole camera model: a 3D point X is projected through the optical center O onto the image plane at pixel coordinates (u,v)."
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/pinhole_model.pdf}
%     \caption{The pinhole camera model geometry showing the projection of a 3D world point onto the 2D image plane.}
%     \label{fig:pinhole_model}
% \end{figure}
% =============================================

\subsubsection{Lens Distortion}

Real camera lenses deviate from the ideal pinhole model by introducing geometric distortions. The two primary types are:

\textbf{Radial distortion} causes straight lines to appear curved, particularly near the image periphery. It is modeled as a polynomial function of the distance $r$ from the principal point:
\begin{equation}
    \begin{aligned}
        x_{distorted} & = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\
        y_{distorted} & = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
    \end{aligned}
    \label{eq:radial_distortion}
\end{equation}
where $(x, y)$ are normalized image coordinates and $k_1, k_2, k_3$ are the radial distortion coefficients. Positive coefficients produce barrel distortion (lines curve outward), while negative coefficients produce pincushion distortion (lines curve inward).

\textbf{Tangential distortion} arises from imperfect alignment between the lens elements and the image sensor, causing asymmetric displacement:
\begin{equation}
    \begin{aligned}
        x_{distorted} & = x + [2p_1 xy + p_2(r^2 + 2x^2)] \\
        y_{distorted} & = y + [p_1(r^2 + 2y^2) + 2p_2 xy]
    \end{aligned}
    \label{eq:tangential_distortion}
\end{equation}
where $p_1, p_2$ are the tangential distortion coefficients.

For accurate metric measurement, these distortions must be corrected through an \textit{undistortion} process that maps observed pixel coordinates to their ideal positions.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Lens distortion visualization
% Content: Side-by-side comparison showing (a) barrel distortion, (b) pincushion distortion,
% (c) original grid pattern, (d) undistorted result
% Caption: "Types of lens distortion: (a) barrel distortion (k1 > 0), (b) pincushion distortion (k1 < 0), and (c) the effect of undistortion on a checkerboard pattern."
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/distortion_types.pdf}
%     \caption{Visualization of radial lens distortion types and the undistortion correction process.}
%     \label{fig:distortion}
% \end{figure}
% =============================================

\subsubsection{Camera Calibration Techniques}

Camera calibration is the process of estimating the intrinsic parameters $K$ and distortion coefficients from images of known patterns. Zhang's flexible calibration method~\cite{zhang2000flexible} revolutionized this process by requiring only a planar calibration target (typically a checkerboard) captured at multiple orientations.

The procedure involves:
\begin{enumerate}
    \item \textbf{Pattern detection}: Locating grid corners in each calibration image using algorithms such as \texttt{findChessboard\allowbreak Corners}.
    \item \textbf{Subpixel refinement}: Improving corner localization accuracy to 0.1 pixels or better using gradient-based methods (\texttt{cornerSubPix}).
    \item \textbf{Homography estimation}: Computing the $3 \times 3$ homography $H_i$ mapping pattern coordinates to image coordinates for each view.
    \item \textbf{Intrinsic extraction}: Exploiting constraints on the absolute conic to solve for $K$ from multiple homographies.
    \item \textbf{Extrinsic recovery}: Computing rotation and translation for each view given $K$.
    \item \textbf{Nonlinear refinement}: Minimizing the total reprojection error over all views using Levenberg-Marquardt optimization.
\end{enumerate}

The reprojection error---the RMS distance between observed corners and their predicted positions---serves as a quality metric; values below 0.5 pixels indicate excellent calibration.

\subsubsection{Advanced Calibration Targets: ChArUco}
While traditional checkerboards are widely used, they suffer from a significant limitation: they require the entire board to be visible to establish the board coordinate system. This constrains the user to keep the full board in the camera's field of view, making it difficult to collect corner data at the extreme edges of the image where lens distortion is most severe.

To address this, ChArUco boards~\cite{garrido2014charuco} were developed as a hybrid of checkerboards and ArUco fiducial markers (see Figure \ref{fig:charuco}). A ChArUco board places unique ArUco markers inside the white squares of a checkerboard. This design offers two critical advantages:
\begin{enumerate}
    \item \textbf{Partial Occlusion Robustness}: Unlike standard checkerboards, ChArUco boards do not require the entire pattern to be visible. The system can uniquely identify any visible subset of the board using the ArUco IDs, allowing for calibration even when corners are occluded or cut off by the frame edge.
    \item \textbf{Improved Corner Coverage}: Because partial visibility is supported, users can move the board close to the camera boundaries, capturing data points in the high-distortion peripheral regions that are essential for accurate distortion modeling.
    \item \textbf{High Precision}: The checkerboard structure retains the high-precision corner detection properties of grid patterns (using saddle-point refinement), while the ArUco markers provide robust identification.
\end{enumerate}
This work employs ChArUco calibration to ensure robust estimation of distortion coefficients, particularly in the peripheral regions of the smartphone lens.

\begin{figure}[htbp]
     \centering
     % Hình bên trái: ChArUco Board
     \begin{subfigure}[b]{0.64\textwidth}
         \centering
         \includegraphics[width=\textwidth]{charuco_board.png}
         \caption{ChArUco board pattern}
         \label{fig:charuco}
     \end{subfigure}
     \hfill % Tạo khoảng trống giữa hai hình
     % Hình bên phải: ArUco Pattern
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{marker42.png}
         \caption{ArUco marker}
         \label{fig:aruco}
     \end{subfigure}
     
     \caption{Comparison of calibration patterns: (a) ChArUco board combining checkerboard corners with ArUco markers, and (b) standard ArUco marker grid.}
     \label{fig:aruco}
\end{figure}

\subsection{Single-View Metrology}

Single-view metrology addresses the recovery of metric information from a single 2D image using projective geometry constraints~\cite{criminisi2000single}. This is fundamentally an ill-posed problem since depth information is lost during projection; however, additional constraints---such as known geometry, scene planarity, or reference dimensions---can make metric reconstruction tractable.

\subsubsection{Homography and Ground-Plane Measurement}

When measuring objects that lie on a known plane (e.g., the ground or a table surface), the image-to-world mapping simplifies considerably. For a plane at $Z = 0$ in world coordinates, the projection equation reduces to a $3 \times 3$ homography $H$:

\begin{equation}
    s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = H \begin{bmatrix} X \\ Y \\ 1 \end{bmatrix}, \quad H = K [r_1 \mid r_2 \mid t]
    \label{eq:homography}
\end{equation}

where $r_1, r_2$ are the first two columns of the rotation matrix $R$ and $t$ is the translation. Given knowledge of the camera's intrinsic matrix $K$, height $h$ above the ground plane, and orientation from an inertial measurement unit (IMU), the homography $H$ can be computed analytically. The inverse homography $H^{-1}$ then maps any image point $(u, v)$ to ground coordinates $(X, Y)$, enabling direct distance computation:

\begin{equation}
    d = \sqrt{(X_A - X_B)^2 + (Y_A - Y_B)^2}
    \label{eq:ground_distance}
\end{equation}

This approach is particularly effective for measuring horizontal distances on floors, tables, or other flat surfaces when the camera pose is known.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Ground-plane homography
% Content: Diagram showing camera above ground plane, coordinate systems, homography mapping,
% and measurement between two points A and B on the ground
% Caption: "Ground-plane homography: the camera at height h above the ground enables mapping of image points to world coordinates for distance measurement."
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/ground_plane_homography.pdf}
%     \caption{Ground-plane measurement using homography derived from camera height and orientation.}
%     \label{fig:ground_homography}
% \end{figure}
% =============================================

\subsubsection{Planar Object Rectification}

For planar objects not aligned with the ground (e.g., a wall-mounted poster, a box face, or a sheet of paper), perspective rectification enables measurement. Given four corner points of a rectangular object in the image, a homography $H_{rect}$ can be computed to warp the object to a fronto-parallel (orthogonal) view:

\begin{equation}
    H_{rect}: (u_i, v_i) \rightarrow (x'_i, y'_i), \quad i = 1, 2, 3, 4
\end{equation}

In the rectified image, the object appears as a rectangle without perspective distortion. If the approximate distance $Z$ from the camera to the object plane is known (e.g., from autofocus metadata or user input), the pixel-to-metric scale can be derived from the pinhole model:

\begin{equation}
    L_{real} = \frac{L_{pixels} \cdot Z}{f}
    \label{eq:pinhole_scale}
\end{equation}

where $f$ is the focal length and $L_{pixels}$ is the measured length in the rectified image.

\subsubsection{Vanishing Points and Cross-Ratio}

For scenes with strong perspective effects---such as tall buildings, corridors, or streets---vanishing point analysis provides powerful geometric constraints~\cite{cipolla1999camera}. Parallel lines in 3D space converge to a vanishing point (VP) in the image; three mutually orthogonal vanishing points (corresponding to the X, Y, and Z world axes) fully constrain the camera's orientation.

The cross-ratio, a projective invariant preserved under perspective projection, enables height estimation from a single image. Given a vertical structure with its base at ground level, if the camera height $h_{cam}$ is known and the horizon line (connecting horizontal vanishing points) is identified, the ratio of image distances can be used to compute the unknown object height $h_{obj}$:

\begin{equation}
    h_{obj} = h_{cam} \cdot \frac{d(base, top)}{d(base, horizon)}
    \label{eq:cross_ratio}
\end{equation}

where $d(\cdot, \cdot)$ denotes the signed distance along the vertical image direction.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Vanishing points and height estimation
% Content: (a) Image with detected lines and computed vanishing points,
% (b) Horizon line and vertical measurement diagram,
% (c) Cross-ratio geometry for height estimation
% Caption: "Single-view metrology using vanishing points: (a) vanishing point detection from parallel lines, (b) horizon line determination, (c) height estimation using cross-ratio."
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/vanishing_points.pdf}
%     \caption{Vanishing point detection and cross-ratio based height estimation.}
%     \label{fig:vanishing_points}
% \end{figure}
% =============================================

\subsection{Why Not Multi-View Triangulation?}

Multi-view triangulation is a well-established technique for recovering 3D structure from multiple images, capable of obtaining metric depth without requiring explicit scale assumptions such as known distances or camera height. Given corresponding points across two or more views, the 3D position can be computed by intersecting the back-projected rays from each camera.

However, triangulation critically depends on accurate knowledge of the \textit{camera extrinsics}---specifically, the relative pose (rotation $R$ and translation $t$) between frames, commonly referred to as the \textit{baseline}. The accuracy of depth recovery is directly proportional to baseline accuracy; even small errors in the estimated translation lead to large errors in triangulated depth, particularly for distant objects.

In systems employing visual SLAM (e.g., ARCore, ORB-SLAM), the baseline is continuously estimated through feature tracking, epipolar geometry constraints, and bundle adjustment optimization. These frameworks provide robust, drift-corrected pose estimation suitable for metric triangulation. However, in the absence of such frameworks, the only available motion source is the device's integrated inertial measurement unit (IMU).

\subsubsection{IMU-Based Baseline Estimation and Its Limitations}

Estimating camera translation from IMU data requires double-integrating accelerometer readings:
\begin{equation}
    \mathbf{p}(t) = \mathbf{p}_0 + \mathbf{v}_0 t + \int_0^t \int_0^{\tau} (\mathbf{a}(s) - \mathbf{g}) \, ds \, d\tau
\end{equation}
where $\mathbf{a}(t)$ is the measured acceleration and $\mathbf{g}$ is the gravity vector. This double integration is notoriously sensitive to sensor bias and noise:

\begin{itemize}
    \item \textbf{Accelerometer bias}: Even a small constant bias $\epsilon$ accumulates quadratically: position error $\propto \epsilon t^2 / 2$.
    \item \textbf{White noise}: Random noise integrates to a random walk in velocity and a double random walk in position.
    \item \textbf{Gravity alignment}: Any error in estimating the gravity vector directly corrupts the horizontal acceleration estimate.
\end{itemize}

For typical smartphone-grade MEMS IMUs, position drift can exceed several centimeters within 1--2 seconds of free motion. This drift magnitude is comparable to or larger than the baseline distances achievable in handheld measurement scenarios, rendering IMU-only baseline estimates unreliable for metric triangulation.

\subsubsection{Design Decision: Focus on Single-View Methods}

Given these constraints, this work focuses on optimizing \textit{single-view} measurement techniques that impose geometric constraints (ground-plane, planarity, vanishing points) rather than relying on multi-view geometry. This design decision offers several advantages:

\begin{enumerate}
    \item \textbf{No motion requirement}: Measurements can be taken from a single, static camera position---faster and more convenient for users.
    \item \textbf{No drift accumulation}: Each frame is processed independently; there is no error accumulation across time.
    \item \textbf{Full transparency}: The geometric assumptions (camera height, planar surface, known reference) are explicit and controllable.
    \item \textbf{Broader device compatibility}: Works on any device with Camera2 API support, without requiring sophisticated visual-inertial odometry pipelines.
\end{enumerate}



\subsection{Smartphone Camera Systems and the Android Camera2 API}

Modern smartphones integrate sophisticated camera systems that present both opportunities and challenges for metrology applications. This section examines the hardware characteristics and software interfaces relevant to measurement accuracy.

\subsubsection{Smartphone Camera Hardware}

Contemporary smartphone cameras feature:
\begin{itemize}
    \item \textbf{High-resolution sensors}: Typical resolutions of 12--200 megapixels, with pixel sizes ranging from 0.7 to 1.8 $\mu m$.
    \item \textbf{Multi-camera arrays}: Wide, ultra-wide, and telephoto lenses with different intrinsic parameters.
    \item \textbf{Optical Image Stabilization (OIS)}: Physical lens displacement to compensate for hand shake, which can affect the effective optical axis.
    \item \textbf{Autofocus systems}: Phase-detection or contrast-detection AF providing focus distance metadata.
    \item \textbf{Inertial Measurement Units (IMU)}: Accelerometers and gyroscopes providing device orientation.
\end{itemize}

Unlike professional cameras with fixed, well-characterized lenses, smartphone cameras exhibit significant variation in intrinsic parameters across devices and may not provide accurate factory calibration.

\subsubsection{The Camera2 API and Intrinsic Metadata}

The Android Camera2 API~\cite{android_camera2} provides low-level access to camera hardware, exposing metadata critical for geometric applications. Key fields include:

\begin{itemize}
    \item \texttt{LENS\_INTRINSIC\_CALIBRATION}: A 5-element array $(f_x, f_y, c_x, c_y, s)$ providing manufacturer-calibrated intrinsic parameters in pixels, defined relative to the sensor's active array coordinate system.

    \item \texttt{SENSOR\_INFO\_ACTIVE\_ARRAY\_SIZE}: The dimensions $(W_s, H_s)$ of the sensor's light-sensitive area in pixels.

    \item \texttt{SENSOR\_INFO\_PHYSICAL\_SIZE}: The physical dimensions of the active array in millimeters, enabling conversion between pixel and metric focal lengths.

    \item \texttt{LENS\_RADIAL\_DISTORTION}: Radial distortion coefficients (when available; many devices report this as unavailable).

    \item \texttt{LENS\_POSE\_ROTATION} and \texttt{LENS\_POSE\_TRANSLATION}: The pose of the camera relative to the device's IMU coordinate frame.
\end{itemize}

The availability and accuracy of these fields vary significantly across devices. For example, our target device (Xiaomi Poco X6 Pro) provides \texttt{LENS\_INTRINSIC\_CALIBRATION} but reports \texttt{LENS\_RADIAL\_DISTORTION} as unavailable.

\subsubsection{Digital Zoom and the Dynamic Crop Region}

A critical challenge for measurement applications is handling digital zoom. Unlike optical zoom, which physically adjusts lens elements and changes the optical focal length, digital zoom operates entirely in the image processing pipeline by:

\begin{enumerate}
    \item Cropping a subregion of the sensor (the ``crop region'')
    \item Scaling the cropped region to the output resolution through interpolation
\end{enumerate}

The \texttt{SCALER\_CROP\_REGION} field in \texttt{CaptureResult} specifies the active crop rectangle $(x_0, y_0, w_c, h_c)$ for each captured frame. This dynamic cropping fundamentally alters the effective intrinsic parameters:

\begin{itemize}
    \item The \textbf{principal point shifts} from $(c_x, c_y)$ to $(c_x - x_0, c_y - y_0)$ in crop coordinates.
    \item The \textbf{effective focal length scales} by the ratio $s_x = W_{out}/w_c$ (horizontal) and $s_y = H_{out}/h_c$ (vertical).
\end{itemize}

The per-frame intrinsic matrix for the output image becomes:

\begin{equation}
    K_{out} = \begin{bmatrix}
        f_x \cdot s_x & 0             & (c_x - x_0) \cdot s_x \\
        0             & f_y \cdot s_y & (c_y - y_0) \cdot s_y \\
        0             & 0             & 1
    \end{bmatrix}
    \label{eq:dynamic_intrinsics}
\end{equation}

Failure to account for these dynamic changes leads to systematic measurement errors that grow with zoom level---a problem largely overlooked by existing measurement applications.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Digital zoom and crop region
% Content: (a) Full sensor active array with intrinsic coordinate system,
% (b) Crop region at different zoom levels (1x, 2x, 3x),
% (c) Diagram showing how principal point and focal length transform
% Caption: "Dynamic intrinsics under digital zoom: (a) the full sensor active array, (b) crop regions at increasing zoom levels, (c) the transformation of intrinsic parameters."
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/dynamic_intrinsics.pdf}
%     \caption{Digital zoom alters effective intrinsic parameters through sensor cropping and scaling.}
%     \label{fig:dynamic_intrinsics}
% \end{figure}
% =============================================

\subsection{Existing Measurement Approaches}

This section surveys existing approaches to smartphone-based measurement, categorizing them into AR-based and traditional methods.

\subsubsection{AR-Based Measurement Systems}

Commercial measurement applications such as Google's Measure app (using ARCore) and Apple's Measure app (using ARKit) leverage augmented reality frameworks to perform 3D reconstruction in real-time~\cite{arcore_overview}. These systems employ:

\begin{itemize}
    \item \textbf{Visual-Inertial Odometry (VIO)}: Fusing camera images with IMU data to track device motion with centimeter-level accuracy.
    \item \textbf{Simultaneous Localization and Mapping (SLAM)}: Building a sparse 3D map of the environment while simultaneously localizing the camera within it.
    \item \textbf{Plane Detection}: Identifying horizontal and vertical surfaces using geometric analysis of tracked feature points.
    \item \textbf{Depth Estimation}: On supported devices, utilizing time-of-flight (ToF) sensors or stereo cameras to obtain dense depth maps.
\end{itemize}

Users place virtual anchors on detected surfaces and the system computes Euclidean distances in metric world coordinates. Reported accuracies of 1--3\% for objects within 3 meters are common under favorable conditions~\cite{vogt2021ar_accuracy}.

However, AR-based approaches exhibit several fundamental limitations:

\begin{enumerate}
    \item \textbf{Opacity and non-transparency}: The geometric internals are hidden within proprietary frameworks, providing no insight into how measurements are computed. Users cannot inspect, validate, or modify the underlying models.

    \item \textbf{Operational constraints}: SLAM initialization requires camera motion to establish a baseline; tracking can fail in featureless, textureless, or highly reflective environments. The requirement for motion is impractical for quick, single-shot measurements.

    \item \textbf{Computational overhead}: Continuous SLAM processing imposes significant battery and thermal constraints.

    \item \textbf{Device requirements}: ARCore and ARKit are not universally available; older or budget devices may lack certification, limiting accessibility.

    \item \textbf{Environmental dependencies}: Performance degrades in low-light conditions, with moving objects, or on surfaces lacking sufficient texture.
\end{enumerate}

For educational purposes---where understanding the connection between geometric theory (pinhole model, homography, projective invariants) and practical measurement is paramount---such opacity represents a significant barrier.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - AR measurement comparison
% Content: Screenshots of (a) ARCore Measure app in action, (b) typical failure cases
% (featureless surface, reflective floor), (c) comparison table of AR vs traditional approaches
% Caption: "AR-based measurement: (a) Google Measure app interface, (b) failure on reflective surfaces, (c) comparison of AR and traditional approaches."
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/ar_comparison.pdf}
%     \caption{AR-based measurement applications and their operational limitations.}
%     \label{fig:ar_measurement}
% \end{figure}
% =============================================

\subsubsection{Traditional Image-Based Measurement}

Alternative approaches have explored purely image-based measurement on smartphones using classical computer vision techniques:

\begin{itemize}
    \item \textbf{Fixed calibration methods}: Performing offline camera calibration with checkerboard patterns and storing the intrinsic matrix for subsequent use. However, these approaches typically ignore the effects of digital zoom.

    \item \textbf{Reference object methods}: Placing a known-size object (credit card, coin, ruler) in the scene to establish pixel-to-metric scale. While simple, this adds friction to the measurement workflow.

    \item \textbf{IMU-assisted ground-plane estimation}: Using device accelerometers and gyroscopes to estimate camera orientation, combined with a user-provided camera height to construct ground-plane homographies~\cite{mulloni2010handheld}.

    \item \textbf{Vanishing point methods}: Detecting scene geometry from parallel lines to recover camera orientation and apply cross-ratio based measurement~\cite{criminisi2000single}.
\end{itemize}

While these methods are transparent and educational, many exhibit significant limitations in practice:

\begin{enumerate}
    \item \textbf{Static intrinsic assumption}: Most systems assume a fixed camera matrix $K$ and fail to update it when users employ digital zoom, leading to systematic errors.

    \item \textbf{Neglect of Camera2 metadata}: Few systems leverage the rich per-frame metadata available through Camera2, particularly \texttt{SCALER\_CROP\_REGION}.

    \item \textbf{Limited processing options}: Users cannot easily compare the effects of different processing choices (undistortion on/off, different measurement models).

    \item \textbf{Lack of experimental validation}: Rigorous comparison against AR baselines under controlled conditions is rarely provided.
\end{enumerate}

\subsection{Research Gap and Contributions}

The existing literature reveals two significant gaps that this work addresses:

\begin{enumerate}
    \item \textbf{Lack of transparent, educational measurement systems}: Current AR-based tools act as ``black boxes,'' concealing the connection between geometric theory and practical measurement. For learners and researchers seeking to understand, validate, or modify the underlying models, such opacity is a significant barrier. There is a need for a measurement system that exposes every step of the pipeline---from raw sensor data to final metric output---enabling rigorous educational and experimental use.

    \item \textbf{Limited handling of dynamic intrinsics under zoom}: Few systems leverage the rich Camera2 metadata---particularly \texttt{SCALER\_CROP\_REGION}---to maintain correct intrinsic parameters across continuous zoom levels. This oversight results in zoom-dependent errors that compound at higher magnification. No prior work has systematically validated the dynamic intrinsics approach against both ground truth and AR baselines.
\end{enumerate}

Table~\ref{tab:related_work} summarizes the key characteristics of existing approaches compared to the proposed system.

% ============ TABLE PLACEHOLDER ============
% TODO: Add comparison table
\begin{table}[h]
    \centering
    \caption{Comparison of existing measurement approaches with the proposed system.}
    \label{tab:related_work}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Approach}        & \textbf{Transparency} & \textbf{Dynamic Zoom} & \textbf{No AR Required} & \textbf{Researcher Mode} \\
        \midrule
        ARCore/ARKit Apps        & Low                   & N/A (3D)              & No                      & No                       \\
        Fixed Calibration        & High                  & No                    & Yes                     & No                       \\
        Reference Object         & High                  & Partial               & Yes                     & No                       \\
        \textbf{Proposed System} & \textbf{High}         & \textbf{Yes}          & \textbf{Yes}            & \textbf{Yes}             \\
        \bottomrule
    \end{tabular}
\end{table}
% =============================================

This work addresses these gaps through the following contributions:

\begin{enumerate}
    \item A \textbf{Dynamic Intrinsics mechanism} that recalculates the camera matrix $K_{out}$ per-frame based on \texttt{SCALER\_CROP\_REGION} metadata, maintaining measurement accuracy across continuous zoom levels.

    \item A \textbf{transparent measurement pipeline} implementing classical techniques (ground-plane homography, planar rectification, single-view metrology) with full visibility into every processing step.

    \item A dedicated \textbf{Researcher Mode} that exposes all internal parameters, supports toggling of processing options (undistortion, edge snapping, rectification, multi-frame averaging), and enables export of detailed logs for analysis.

    \item \textbf{Systematic experimental validation} demonstrating the importance of Dynamic Intrinsics across zoom levels and quantifying the impact of individual processing options (undistortion, edge snapping).
\end{enumerate}

\section{Proposed System}
\label{sec:system}

This section describes the architecture and implementation of the proposed measurement system. We present the overall system design, the dynamic intrinsics mechanism, the measurement modules, the image processing pipeline, and the dual-mode user interface.

\subsection{Overall Architecture}

The proposed application is implemented in Flutter with native Android modules for camera access (Camera2 API). The architecture follows a modular design that separates concerns and enables flexible configuration. Figure~\ref{fig:architecture} illustrates the system components.

% System Architecture Diagram using TikZ
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=1.2cm and 2.5cm,
        box/.style={rectangle, draw, rounded corners, minimum width=3cm, minimum height=1cm, align=center, font=\footnotesize},
        bigbox/.style={rectangle, draw, rounded corners, minimum width=3.5cm, minimum height=1cm, align=center, font=\footnotesize},
        smallbox/.style={rectangle, draw, rounded corners, minimum width=2cm, minimum height=0.8cm, align=center, font=\scriptsize},
        arrow/.style={-{Stealth[scale=0.8]}, thick}
        ]

        % Flutter UI Layer
        \node[bigbox, fill=blue!15] (ui) {Flutter UI\\(User/Researcher Mode)};

        % Platform Channel
        \node[box, below=1.2cm of ui, fill=yellow!20] (channel) {Platform Channel\\(MethodChannel)};

        % Native Module
        \node[box, below=1.2cm of channel, fill=green!15] (camera2) {Camera2 Module\\(Frames + Metadata)};

        % IMU - positioned above-left of camera2
        \node[smallbox, left=1.5cm of camera2, fill=purple!10] (imu) {IMU Sensors};

        % Processing Layer
        \node[box, below=1.2cm of camera2, fill=cyan!15] (dynK) {Dynamic Intrinsics\\$K_{out}$ Computation};

        % Measurement Pipeline
        \node[bigbox, below=1.2cm of dynK, fill=teal!15] (pipeline) {Measurement Pipeline\\(Modules A/B/C)};

        % Result
        \node[box, below=1.2cm of pipeline, fill=gray!15] (result) {Result \& Error\\Estimation};

        % Arrows - main flow
        \draw[arrow] (ui) -- (channel);
        \draw[arrow] (channel) -- (camera2);
        \draw[arrow] (imu) -- (camera2);
        \draw[arrow] (camera2) -- (dynK);
        \draw[arrow] (dynK) -- (pipeline);
        \draw[arrow] (pipeline) -- (result);

    \end{tikzpicture}
    \caption{Overall system architecture showing the Flutter application layer, native Camera2 module, and the traditional measurement pipeline. The system computes per-frame dynamic intrinsics and applies classical geometric measurement techniques.}
    \label{fig:architecture}
\end{figure}

The system implements the traditional measurement flow:

\begin{enumerate}
    \item \textbf{Traditional Flow (main contribution)}: Uses Camera2 to capture frames and per-frame metadata, computes dynamic intrinsics, and applies classical geometric measurement techniques.
\end{enumerate}

\subsubsection{Data Acquisition Layer}

The data acquisition layer interfaces with the device hardware through the Android Camera2 API and sensor framework:

\begin{itemize}
    \item \textbf{Camera2 Session}: Captures frames in YUV or JPEG format at configurable resolutions. Each frame is accompanied by a \texttt{CaptureResult} containing real-time metadata. Due to limitations of the standard Flutter camera plugin, direct control over exposure parameters such as ISO, shutter speed, and white balance is not exposed at the API level. Instead, the system locks focus and exposure after auto-convergence to ensure consistency across captured frames. For geometric calibration and measurement tasks, this approach is sufficient, as the pipeline relies on spatial accuracy rather than photometric precision.

    \item \textbf{Camera Characteristics}: Provides static device information including \texttt{LENS\_INTRINSIC\_CALIBRATION}, \texttt{SENSOR\_INFO\_ACTIVE\_ARRAY\_SIZE}, and \texttt{SENSOR\_INFO\_PHYSICAL\_SIZE}.

    \item \textbf{IMU Sensors}: Accelerometer and gyroscope data are fused using a complementary filter to provide device orientation (roll, pitch, yaw) relative to gravity.

    \item \textbf{Platform Channel}: A Flutter MethodChannel bridges the native Android code and the Dart application layer, passing frame data, metadata, and measurement results.
\end{itemize}

\subsection{Camera Model and Dynamic Intrinsics}

The core innovation of this system is the \textit{Dynamic Intrinsics} mechanism that maintains correct camera parameters across continuous digital zoom levels.

\subsubsection{Intrinsic Parameter Sources}

The system supports two sources of intrinsic parameters:

\begin{enumerate}
    \item \textbf{Device Intrinsics (Default)}: Uses the manufacturer-provided \texttt{LENS\_INTRINSIC\_CALIBRATION} from Camera2, which provides $(f_x, f_y, c_x, c_y, s)$ in the sensor's active array coordinate system. Distortion coefficients from \texttt{LENS\_RADIAL\_DISTORTION} are used when available.

    \item \textbf{Custom Calibration (Researcher Mode)}: Users can perform offline calibration using a ChArUco board displayed on a rigid, flat surface to ensure planarity. This approach is more convenient than printing and ensures perfect planarity. The calibration procedure yields custom $K_{self}$ and distortion coefficients. This is particularly important for devices like the Poco X6 Pro, which reports \texttt{LENS\_RADIAL\_DISTORTION} as unavailable (null), requiring either custom calibration or estimated coefficients (e.g., $k_1 \approx -0.08$) for high-precision tasks.
\end{enumerate}

Experimental validation reveals that for modern devices like the Poco X6 Pro, the manufacturer-provided intrinsics are highly accurate, often differing from custom-calibrated values by less than 0.5\%. Consequently, custom calibration is treated as an optional refinement for Researcher Mode, while User Mode relies on the robust Device Intrinsics.

\subsubsection{Dynamic Intrinsics Computation}

When digital zoom is applied, the camera crops a subregion of the sensor and scales it to the output resolution. The \texttt{SCALER\_CROP\_REGION} field in each \texttt{CaptureResult} specifies the active crop rectangle. The dynamic intrinsics are computed as follows:

Let $(W_s, H_s)$ denote the sensor's active array dimensions, $(f_{x,s}, f_{y,s}, c_{x,s}, c_{y,s})$ the device intrinsics in active array coordinates, and $(x_0, y_0, w_c, h_c)$ the current crop region. For output image dimensions $(W_{out}, H_{out})$:

\textbf{Step 1: Transform to crop coordinates.}
\begin{equation}
    c_{x,c} = c_{x,s} - x_0, \quad c_{y,c} = c_{y,s} - y_0
\end{equation}

\textbf{Step 2: Compute scale factors.}
\begin{equation}
    s_x = \frac{W_{out}}{w_c}, \quad s_y = \frac{H_{out}}{h_c}
\end{equation}

\textbf{Step 3: Compute output intrinsics.}
\begin{equation}
    f_{x,out} = f_{x,s} \cdot s_x, \quad f_{y,out} = f_{y,s} \cdot s_y
\end{equation}
\begin{equation}
    c_{x,out} = c_{x,c} \cdot s_x, \quad c_{y,out} = c_{y,c} \cdot s_y
\end{equation}

The resulting per-frame intrinsic matrix is:
\begin{equation}
    K_{out} = \begin{bmatrix}
        f_{x,out} & 0         & c_{x,out} \\
        0         & f_{y,out} & c_{y,out} \\
        0         & 0         & 1
    \end{bmatrix}
    \label{eq:kout_final}
\end{equation}

This matrix is recomputed for every captured frame, ensuring that measurements remain accurate regardless of zoom level.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Dynamic intrinsics computation
% Content: Diagram showing (a) full active array with intrinsic coordinates,
% (b) crop region at zoom 2x, (c) output image with transformed intrinsics
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/dynamic_intrinsics_computation.pdf}
%     \caption{Dynamic intrinsics computation: the crop region transforms the intrinsic parameters from sensor coordinates to output image coordinates.}
%     \label{fig:dynamic_intrinsics_comp}
% \end{figure}
% =============================================

\subsection{Measurement Modules}

The system implements four measurement modules, each targeting different scene configurations and geometric assumptions. Figure~\ref{fig:pipeline_flowchart} illustrates the complete measurement pipeline from input to output.

\subsubsection{Module A: Ground-Plane Measurement}

This module measures distances between points lying on a horizontal ground plane (floor, table). The approach constructs a homography between the image and the ground plane using:

\begin{itemize}
    \item Camera height $h$ above the ground (user-provided or measured)
    \item Camera orientation $R$ from IMU sensors
    \item Intrinsic matrix $K_{out}$
\end{itemize}

The ground plane is defined as $Z = 0$ in world coordinates with normal $\mathbf{n} = (0, 0, 1)^T$. The camera position is $\mathbf{t} = (0, 0, h)^T$. The homography from ground to image is:
\begin{equation}
    H = K_{out} [r_1 \mid r_2 \mid \mathbf{t}_{proj}]
\end{equation}
where $r_1, r_2$ are the first two columns of $R$.

To measure, the user selects two points $A(u_A, v_A)$ and $B(u_B, v_B)$ on the image. These are mapped to ground coordinates via:
\begin{equation}
    (X_A, Y_A) = H^{-1}(u_A, v_A), \quad (X_B, Y_B) = H^{-1}(u_B, v_B)
\end{equation}

The metric distance is:
\begin{equation}
    d = \sqrt{(X_A - X_B)^2 + (Y_A - Y_B)^2}
\end{equation}

For accurate ground-plane measurements, the camera's pitch angle must be precisely estimated. The IMU provides roll and pitch relative to gravity, which are crucial for establishing the ground plane homography. The user is guided to hold the device as steadily as possible, and the system provides visual feedback on orientation stability.

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Ground-plane measurement workflow
% Content: (a) User holding phone above ground, (b) image with selected points,
% (c) ground plane projection diagram
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/ground_plane_measurement.pdf}
%     \caption{Ground-plane measurement: (a) physical setup, (b) point selection on image, (c) geometric projection.}
%     \label{fig:ground_plane_module}
% \end{figure}
% =============================================

\subsubsection{Module B: Planar Object Measurement}

This module measures dimensions of planar objects (paper, screens, box faces) that may not be aligned with the ground. The workflow involves:

\begin{enumerate}
    \item \textbf{Corner Detection}: The user selects four corners of a rectangular object, or the system auto-detects them using Canny edge detection, Hough lines, contour approximation, and subpixel corner refinement.

    \item \textbf{Perspective Rectification}: A homography $H_{rect}$ warps the quadrilateral to a fronto-parallel rectangle:
          \begin{equation}
              H_{rect}: (u_i, v_i) \rightarrow (x'_i, y'_i), \quad i = 1, \ldots, 4
          \end{equation}

    \item \textbf{Scale Estimation}: The pixel-to-metric scale requires knowledge of the object-to-camera distance $Z$. This distance can be obtained from:
          \begin{itemize}
              \item \texttt{LENS\_FOCUS\_DISTANCE} metadata from Camera2 (reciprocal of focus distance in diopters)
              \item User-provided estimate (e.g., ``approximately 40 cm'')
              \item Fixed experimental setup with known camera-to-object distance
          \end{itemize}
          Given $Z$, the pixel-to-metric scale is:
          \begin{equation}
              \text{scale} = \frac{Z}{f_{out}}
          \end{equation}

    \item \textbf{Measurement}: Distances in the rectified image are converted to metric units using the computed scale.
\end{enumerate}

\subsubsection{Module C: Single-View Metrology}

This module targets vertical structures (doors, walls, buildings) using vanishing point analysis:

\begin{enumerate}
    \item \textbf{Line Detection}: Canny edge detection followed by Hough transform or Line Segment Detector (LSD) extracts line segments.

    \item \textbf{Vanishing Point Estimation}: Lines are clustered by orientation; each cluster's intersection yields a vanishing point. RANSAC ensures robustness to outliers.

    \item \textbf{Horizon Line}: The horizon is determined from horizontal vanishing points, or estimated from IMU orientation.

    \item \textbf{Height Estimation}: Using the cross-ratio invariant, the height of a vertical object is computed as:
          \begin{equation}
              h_{obj} = h_{cam} \cdot \frac{d(base, top)}{d(base, horizon)}
          \end{equation}
          where $h_{cam}$ is the known camera height.
\end{enumerate}



% Measurement Pipeline Flowchart using TikZ
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=1cm,
        process/.style={rectangle, draw, rounded corners, minimum width=5cm, minimum height=0.9cm, align=center, font=\footnotesize, fill=blue!10},
        decision/.style={diamond, draw, aspect=2, inner sep=2pt, align=center, font=\scriptsize, fill=yellow!15},
        module/.style={rectangle, draw, rounded corners, minimum width=2.2cm, minimum height=0.8cm, align=center, font=\scriptsize},
        arrow/.style={-{Stealth[scale=0.7]}, thick},
        io/.style={trapezium, trapezium left angle=70, trapezium right angle=110, draw, minimum width=4cm, minimum height=0.8cm, align=center, font=\footnotesize, fill=green!10}
        ]

        % Input
        \node[io] (input) {Camera Frame + Metadata};

        % Dynamic K
        \node[process, below=1cm of input] (dynk) {Compute Dynamic $K_{out}$ from SCALER\_CROP\_REGION};

        % IMU - to the right
        \node[process, right=2cm of dynk, fill=purple!10, minimum width=3cm] (imu) {IMU Orientation\\(Roll, Pitch, Yaw)};

        % Preprocessing
        \node[process, below=1cm of dynk] (preprocess) {Preprocessing (Undistort, Denoise, Edge Detection)};

        % Module Selection
        \node[decision, below=1.2cm of preprocess] (select) {Select\\Module};

        % Modules - horizontal row with proper spacing
        \node[module, below=1.5cm of select, xshift=-3cm, fill=teal!15] (modA) {Module A\\Ground-Plane};
        \node[module, below=1.5cm of select, xshift=0cm, fill=cyan!15] (modB) {Module B\\Planar Object};
        \node[module, below=1.5cm of select, xshift=3cm, fill=orange!15] (modC) {Module C\\Single-View};

        % Merge
        \node[process, below=3cm of select] (result) {Compute Distance \& Error Estimate};

        % Output
        \node[io, below=1cm of result] (output) {Display Result (e.g., 23.4 cm $\pm$ 0.5 cm)};

        % Arrows - main flow
        \draw[arrow] (input) -- (dynk);
        \draw[arrow] (imu) -- (preprocess);
        \draw[arrow] (dynk) -- (preprocess);
        \draw[arrow] (preprocess) -- (select);

        % Arrows from decision to modules
        \draw[arrow] (select.south) -- ++(0,-0.4) -| (modA.north);
        \draw[arrow] (select.south) -- ++(0,-0.4) -| (modB.north);
        \draw[arrow] (select.south) -- ++(0,-0.4) -| (modC.north);

        % Arrows from modules to result
        \draw[arrow] (modA.south) -- ++(0,-0.3) -| (result.north west);
        \draw[arrow] (modB.south) -- ++(0,-0.3) -| (result.north);
        \draw[arrow] (modC.south) -- ++(0,-0.3) -| (result.north east);

        \draw[arrow] (result) -- (output);

    \end{tikzpicture}
    \caption{Traditional measurement pipeline flowchart. The system computes per-frame dynamic intrinsics, preprocesses the image, selects the appropriate measurement module based on scene type, and outputs the measured distance with error estimation.}
    \label{fig:pipeline_flowchart}
\end{figure}

\subsubsection{Device Orientation Requirements}

Each measurement module imposes specific constraints on device orientation to ensure geometric validity. We utilize the Android sensor coordinate system where Roll is rotation around the Y-axis (longitudinal) and Pitch is rotation around the X-axis (lateral).

\begin{table}[h]
    \centering
    \caption{Device orientation requirements for measurement modules.}
    \label{tab:orientation_reqs}
    \begin{tabular}{llcc}
        \toprule
        \textbf{Module}           & \textbf{Orientation Mode} & \textbf{Roll Req.}      & \textbf{Pitch Req.}        \\
        \midrule
        \textbf{A: Ground-Plane}  & Portrait (Camera Down)    & $0^\circ \pm 5^\circ$   & $-30^\circ$ to $-60^\circ$ \\
        \textbf{B: Planar Object} & Flexible                  & Flexible                & Flexible                   \\
        \textbf{C: Single-View}   & Portrait (Upright)        & $90^\circ \pm 10^\circ$ & $\approx 0^\circ$          \\
        \bottomrule
    \end{tabular}
\end{table}

Module A is particularly sensitive to Roll errors; a non-zero Roll significantly distorts the ground-plane homography. To assist users, the UI implements a tiered validation system (Excellent/Good/Poor/Invalid) with a visual bubble level indicator, providing real-time feedback and preventing capture when orientation is invalid.

\subsection{Image Processing Pipeline}

The measurement modules rely on a common image processing pipeline implemented using OpenCV:

\begin{enumerate}
    \item \textbf{Preprocessing}:
          \begin{itemize}
              \item Color to grayscale conversion
              \item Optional undistortion using precomputed maps (if distortion coefficients are available)
              \item Gaussian or bilateral filtering for noise reduction
          \end{itemize}

    \item \textbf{Edge and Line Detection}:
          \begin{itemize}
              \item Canny edge detector with adaptive thresholding
              \item Hough transform or LSD for line extraction
              \item RANSAC-based line fitting for robustness
          \end{itemize}

    \item \textbf{Contour and Shape Detection}:
          \begin{itemize}
              \item Binary thresholding with morphological operations (opening/closing)
              \item \texttt{findContours} and \texttt{approxPolyDP} for polygon approximation
              \item \texttt{minAreaRect} for bounding rectangle estimation
          \end{itemize}

    \item \textbf{Corner Refinement}:
          \begin{itemize}
              \item Harris or Shi-Tomasi corner detection
              \item \texttt{cornerSubPix} for subpixel accuracy
              \item Line intersection computation for geometric corners
          \end{itemize}

\end{enumerate}

\paragraph{Edge-Based Point Snapping}
Edge snapping is treated strictly as a user-interaction refinement technique and is applied only on frozen frames. It does not alter the underlying geometric measurement model. Its sole purpose is to reduce user-induced variance by snapping manually selected points to nearby detected edges or line intersections, thereby improving repeatability without introducing systematic bias.

\subsection{User Interface and Modes}

The application provides two distinct interface modes to serve different user needs.

\subsubsection{User Mode}

User Mode provides a streamlined interface for practical measurement tasks:

\begin{itemize}
    \item \textbf{Measurement Type Selection}: Simple choices such as ``Measure on Floor/Table'' or ``Measure Flat Object''.
    \item \textbf{Camera Preview}: Real-time viewfinder with pinch-to-zoom support.
    \item \textbf{Point Selection Modes}:
          \begin{itemize}
              \item \textit{Live Preview}: Allows quick selection of up to 2 points for real-time estimation.
              \item \textit{Frozen Image}: Captures a high-resolution static frame, allowing users to zoom in and precisely select multiple point pairs. This mode is preferred for accuracy as it eliminates hand tremor during selection.
          \end{itemize}
    \item \textbf{Automatic Processing}: Device intrinsics are used by default; dynamic intrinsics are computed transparently.
    \item \textbf{Result Display}: Measurement result shown with estimated uncertainty (e.g., ``23.4 cm $\pm$ 0.5 cm'').
\end{itemize}

\subsubsection{Researcher Mode}

Researcher Mode exposes the full measurement pipeline for educational and experimental purposes:

\begin{itemize}
    \item \textbf{Intrinsics Configuration}:
          \begin{itemize}
              \item Toggle between Device Intrinsics and Custom Calibration
              \item Display current $K_{out}$, crop region, and zoom level
              \item Access to calibration playground for custom ChArUco calibration
          \end{itemize}

    \item \textbf{Processing Options}:
          \begin{itemize}
              \item Toggle undistortion on/off
              \item Toggle edge-based point snapping
              \item Toggle perspective rectification
          \end{itemize}

    \item \textbf{Measurement Model Selection}:
          \begin{itemize}
              \item Ground-plane homography
              \item Planar object + pinhole
              \item Single-view metrology
          \end{itemize}

    \item \textbf{Debug Overlay}:
          \begin{itemize}
              \item Principal point and image axes
              \item Grid overlay
              \item Detected vanishing points and lines
              \item IMU orientation (pitch, roll, yaw)
              \item Current $K_{out}$ values
          \end{itemize}

    \item \textbf{Logging and Export}:
          \begin{itemize}
              \item Detailed logs for each measurement (timestamp, mode, $K_{out}$, orientation, result, configuration flags)
              \item Export to JSON/CSV for offline analysis
          \end{itemize}
\end{itemize}

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - User interface screenshots
% Content: (a) User Mode main screen, (b) User Mode measurement result,
% (c) Researcher Mode settings panel, (d) Researcher Mode debug overlay
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/ui_modes.pdf}
%     \caption{Application interface: (a-b) User Mode for practical measurement, (c-d) Researcher Mode with full control and debug overlay.}
%     \label{fig:ui_modes}
% \end{figure}
% =============================================

% \subsection{ARCore Baseline Implementation}
%
% To provide a reference for comparison, an ARCore-based measurement flow is implemented. Since the primary development device (Xiaomi Poco X6 Pro) does not support ARCore, this baseline runs on an ARCore-certified device (Google Pixel 7 Pro) for experimental comparison. The ARCore flow operates as follows:
%
% \begin{itemize}
%     \item \textbf{Session Initialization}: ARCore initializes visual-inertial tracking and begins building a sparse 3D map.
%
%     \item \textbf{Plane Detection}: Horizontal and vertical surfaces are detected and visualized as meshes. The Pixel 7 Pro's ToF sensor enhances depth estimation.
%
%     \item \textbf{Anchor Placement}: The user taps on detected planes to place 3D anchors at hit-test positions.
%
%     \item \textbf{Distance Computation}: The Euclidean distance between two anchors is computed directly in metric world coordinates:
%           \begin{equation}
%               d_{AR} = \|\mathbf{p}_A - \mathbf{p}_B\|_2
%           \end{equation}
%           where $\mathbf{p}_A, \mathbf{p}_B \in \mathbb{R}^3$ are anchor positions.
% \end{itemize}
%
% This baseline provides a practical comparison point but operates as a ``black box''---the internal tracking, calibration, and depth estimation details are hidden within the ARCore SDK. Importantly, this ARCore flow is unavailable on the Poco X6 Pro and many other Android devices, which motivates the development of the traditional, Camera2-based pipeline as a universally-accessible alternative.

% ARCore Baseline Implementation section removed - focusing on traditional pipeline only
% The traditional Camera2-based pipeline is the main contribution of this work
% and does not require ARCore comparison for validation.

%==============================================================================
\section{Experimental Setup}
\label{sec:experiment}
This section describes the experimental methodology used to evaluate the proposed measurement system. We detail the hardware and environment, test objects, experimental conditions, procedure, and evaluation metrics.

\subsection{Hardware and Environment}

\subsubsection{Test Device}

Experiments were conducted using a Xiaomi Poco X6 Pro smartphone:

\paragraph{Xiaomi Poco X6 Pro}

This device was used for all measurement experiments:

\begin{itemize}
    \item \textbf{Operating System}: Android 14 (HyperOS)
    \item \textbf{Main Camera}: 64 MP, f/1.89, 26mm equivalent, PDAF
    \item \textbf{Sensor}: 1/2'' OmniVision OV64B with 0.7 $\mu m$ pixels (binned to 1.4 $\mu m$)
    \item \textbf{Camera2 API Level}: FULL (supports \texttt{LENS\_INTRINSIC\_CALIBRATION})
    \item \textbf{ARCore Support}: \textit{Not supported} (device not certified)
    \item \textbf{IMU}: 6-axis accelerometer and gyroscope
\end{itemize}

The device's Camera2 metadata availability was verified:
\begin{itemize}
    \item \texttt{LENS\_INTRINSIC\_CALIBRATION}: Available
    \item \texttt{SENSOR\_INFO\_ACTIVE\_ARRAY\_SIZE}: 9248 $\times$ 6936 pixels
    \item \texttt{LENS\_RADIAL\_DISTORTION}: Unavailable (reported as null)
    \item \texttt{SCALER\_CROP\_REGION}: Available per-frame
\end{itemize}

\paragraph{Device Selection Rationale}

The Poco X6 Pro was chosen because it represents a modern mid-range Android device with full Camera2 API support but without ARCore certification. This is precisely the target use case for the proposed traditional pipeline: devices with capable camera hardware but without access to AR-based measurement tools. The lack of \texttt{LENS\_RADIAL\_DISTORTION} metadata also provides an opportunity to evaluate the impact of custom calibration.

\subsubsection{Test Environment}

Experiments were conducted in a controlled indoor environment:

\begin{itemize}
    \item \textbf{Lighting}: Stable artificial lighting ($\sim$500 lux), avoiding direct sunlight and shadows
    \item \textbf{Surfaces}: Flat floor (laminate) and table surfaces (wood) for ground-plane experiments
    \item \textbf{Background}: Textured walls and floor for visual reference
    \item \textbf{Temperature}: Room temperature ($\sim$25°C) to minimize thermal drift in IMU
\end{itemize}

\subsection{Test Objects}

We selected a variety of test objects with precisely known dimensions:

\begin{table}[h]
    \centering
    \caption{Test objects and their ground-truth dimensions.}
    \label{tab:test_objects}
    \begin{tabular}{llc}
        \toprule
        \textbf{Object}        & \textbf{Dimensions}              & \textbf{Measurement Type} \\
        \midrule
        A4 Paper               & 210 $\times$ 297 mm              & Planar object             \\
        Metal Ruler            & 300 mm length                    & Ground-plane / Planar     \\
        Cardboard Box (small)  & 150 $\times$ 100 $\times$ 80 mm  & Planar object             \\
        Cardboard Box (medium) & 300 $\times$ 200 $\times$ 150 mm & Ground-plane              \\
        Book                   & 240 $\times$ 170 mm cover        & Planar object             \\
        Door Frame             & 2000 mm height                   & Single-view metrology     \\
        Floor Tile             & 400 $\times$ 400 mm              & Ground-plane              \\
        \bottomrule
    \end{tabular}
\end{table}

Ground-truth measurements were obtained using manual measurement tools (metal ruler or measuring tape), which provide millimeter-level accuracy and are sufficient for validating the proposed monocular measurement system.

\subsection{Experimental Conditions}

We systematically varied the following experimental parameters:

\subsubsection{Distance to Object}

Measurements were performed at multiple distances from the camera to the target object:
\begin{itemize}
    \item \textbf{Close range}: 0.3 m, 0.4 m
    \item \textbf{Medium range}: 0.6 m, 0.8 m, 1.0 m
    \item \textbf{Far range}: 1.5 m, 2.0 m, 3.0 m
\end{itemize}

\subsubsection{Zoom Levels}

Digital zoom was varied to test the Dynamic Intrinsics mechanism:
\begin{itemize}
    \item 1.0$\times$ (no zoom, full sensor crop)
    \item 1.5$\times$
    \item 2.0$\times$
    \item 3.0$\times$
    \item 4.0$\times$ (maximum stable zoom)
\end{itemize}

\subsubsection{Viewing Angles}

For planar object measurements, the camera angle relative to the object plane was varied:
\begin{itemize}
    \item \textbf{Frontal}: $\sim$0° (perpendicular to object)
    \item \textbf{Moderate oblique}: $\sim$30° off-normal
    \item \textbf{Steep oblique}: $\sim$45° off-normal
\end{itemize}

\subsubsection{Target Accuracy}

Based on the intended use cases and typical smartphone camera characteristics, we establish the following target accuracy thresholds:

\begin{itemize}
    \item \textbf{Close to medium range} (0.3--1.0 m): Relative error $< 5\%$ for objects 20--50 cm in size.
    \item \textbf{Far range} (1.0--3.0 m): Relative error $< 10\%$ for larger objects.
\end{itemize}

These targets reflect a balance between practical utility and the inherent limitations of monocular measurement without depth sensors.

\subsubsection{Method Configurations}

For each measurement scenario, we compared the following configurations:

\begin{enumerate}
    \item \textbf{Dynamic Intrinsics, No Undistortion}: Uses \texttt{LENS\_INTRINSIC\_CALIBRATION} with dynamic crop adjustment; undistortion disabled.

    \item \textbf{Dynamic Intrinsics, With Undistortion}: Same as above but with undistortion enabled (using estimated coefficients from custom calibration).

    \item \textbf{Custom Calibration}: Uses self-calibrated $K$ and distortion coefficients from ChArUco calibration.

    \item \textbf{Fixed K (Control)}: Uses a fixed intrinsic matrix ignoring crop region changes---demonstrates the importance of dynamic intrinsics.
\end{enumerate}

\subsection{Experimental Procedure}

\subsubsection{Calibration Phase (One-time)}

Before the main experiments, custom calibration was performed:

\begin{enumerate}
    \item A ChArUco board (5$\times$7 squares, 30 mm square size, 22 mm marker size) was printed on rigid foam board.
    \item 35 images were captured at varying distances (0.3--1.0 m) and orientations (rotations up to 45°).
    \item OpenCV's \texttt{calibrateCameraCharuco} was used to compute $K_{self}$ and distortion coefficients.
    \item Achieved reprojection error: 0.38 pixels RMS.
    \item Calibration results were stored as JSON for use in Researcher Mode.
\end{enumerate}

\subsubsection{Measurement Trials}

For each object, distance, zoom level, and configuration combination:

\begin{enumerate}
    \item The test object was placed on a flat surface with a measuring reference nearby.
    \item The camera-to-object distance was kept approximately constant using a marked reference distance.
    \item Camera height was measured and entered into the application (for ground-plane mode).
    \item The application was launched in the appropriate measurement mode.
    \item The zoom level was set using pinch gesture.
    \item The user tapped two endpoints of the target dimension.
    \item The measurement result was recorded.
    \item Each measurement was repeated $N = 5$ times to assess variability.
    \item All data (configuration, $K_{out}$, crop region, IMU orientation, result) were logged.
\end{enumerate}

\subsubsection{Data Collection Summary}

The full experimental matrix comprised:
\begin{itemize}
    \item 7 test objects
    \item 5 distance levels (where applicable)
    \item 5 zoom levels
    \item 4 method configurations
    \item 5 repetitions per condition
\end{itemize}

This resulted in several hundred individual measurements across all conditions.

\subsection{Evaluation Metrics}

Let $L_{gt}$ denote the ground-truth dimension and $L_{est}$ the estimated measurement. For each trial, we compute:

\subsubsection{Absolute Error}
\begin{equation}
    E_{abs} = |L_{est} - L_{gt}| \quad \text{(mm or cm)}
\end{equation}

\subsubsection{Relative Error}
\begin{equation}
    E_{rel} = \frac{|L_{est} - L_{gt}|}{L_{gt}} \times 100\%
\end{equation}

\subsubsection{Aggregate Statistics}

For each configuration and condition, we report:
\begin{itemize}
    \item \textbf{Mean Absolute Error (MAE)}: Average of $E_{abs}$ across repetitions
    \item \textbf{Mean Relative Error (MRE)}: Average of $E_{rel}$ across repetitions
    \item \textbf{Standard Deviation ($\sigma$)}: Variability across repetitions
    \item \textbf{Maximum Error}: Worst-case error observed
\end{itemize}

\subsubsection{Comparative Analysis}

Results are analyzed along several dimensions:

\begin{enumerate}
    \item \textbf{Error vs. Distance}: How does accuracy degrade with increasing object distance?

    \item \textbf{Error vs. Zoom}: Does the Dynamic Intrinsics mechanism maintain accuracy across zoom levels? How does Fixed K compare?

    \item \textbf{Error vs. Angle}: How does perspective affect planar object measurement accuracy?

    \item \textbf{Dynamic vs. Fixed Intrinsics}: Quantifying the importance of per-frame $K_{out}$ computation.

    \item \textbf{Effect of Processing Options}: Quantifying the impact of undistortion, edge snapping, and rectification.
\end{enumerate}

% ============ FIGURE PLACEHOLDER ============
% TODO: Add Figure - Experimental setup photos
% Content: (a) Test device with app running, (b) test objects arranged,
% (c) measurement setup with distance marker, (d) ChArUco calibration board
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/experimental_setup.pdf}
%     \caption{Experimental setup: (a) Xiaomi Poco X6 Pro test device, (b) test objects with known dimensions, (c) distance measurement setup, (d) ChArUco calibration board.}
%     \label{fig:exp_setup}
% \end{figure}
% =============================================

%==============================================================================
\section{Results}
\label{sec:results}

This section presents the quantitative evaluation of the proposed system. All measurements were conducted on a Xiaomi Poco X6 Pro, with ground-truth compared against manual measurements using a metal ruler or measuring tape.

\subsection{Overall Measurement Accuracy}

Table~\ref{tab:overall_accuracy} summarizes the aggregate performance of the system across all test objects at a standard distance of 0.6\,m. The Dynamic Intrinsics method achieved a mean absolute error (MAE) ranging from 4.8\,mm to 9.2\,mm, satisfying our target accuracy for practical measurement applications.

\begin{table}[h]
    \centering
    \caption{Overall accuracy summary at 0.6\,m distance (1.0$\times$ zoom).}
    \label{tab:overall_accuracy}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Method Configuration} & \textbf{MAE (mm)} & \textbf{MRE (\%)} & \textbf{Max Error (mm)} & \textbf{Std Dev (mm)} \\ \midrule
        Module A (Ground-Plane)       & 8.4               & 2.8\%             & 12.5                    & 3.2                   \\
        Module B (Planar Object)      & 5.2               & 1.7\%             & 9.8                     & 2.1                   \\
        Module C (Single-View)        & 9.2               & 3.1\%             & 15.4                    & 4.5                   \\
        \textbf{Mean Aggregate}       & \textbf{7.6}      & \textbf{2.5\%}    & \textbf{12.6}           & \textbf{3.3}          \\ \bottomrule
    \end{tabular}
\end{table}

\subsection{Effect of Distance}

The accuracy of monocular measurement is inherently sensitive to the distance between the camera and the target. As shown in Figure~\ref{fig:error_distance_plot}, the Mean Relative Error (MRE) remains below 3\% for distances under 1.0\,m but increases to approximately 6.5\% at 3.0\,m. This trend is attributed to the reduced pixel-per-metric density and the increased impact of small angular errors in IMU orientation at longer ranges.

% Gợi ý: Bạn có thể dùng pgfplots để vẽ biểu đồ này trực tiếp trong LaTeX
\begin{figure}[htbp]
    \centering
    \caption{Mean Relative Error vs. Distance. Error remains within the 0.5--1.0\,cm range for typical indoor distances.}
    \label{fig:error_distance_plot}
    % [Chèn file PDF/PNG biểu đồ tại đây]
\end{figure}

\subsection{Effect of Digital Zoom: Dynamic vs. Fixed Intrinsics}

A core contribution of this work is the Dynamic Intrinsics mechanism. Table~\ref{tab:zoom_comparison} illustrates the critical failure of the Fixed $K$ approach when digital zoom is applied. While Dynamic Intrinsics maintains a stable error rate (MAE $\approx$ 7--9\,mm), the Fixed $K$ error grows exponentially, reaching over 70\% at 4.0$\times$ zoom.

\begin{table}[h]
    \centering
    \caption{Comparison of accuracy across zoom levels (Target: 300\,mm Metal Ruler).}
    \label{tab:zoom_comparison}
    \begin{tabular}{@{}ccccc@{}}
        \toprule
        & \multicolumn{2}{c}{\textbf{Fixed $K$ (Control)}} & \multicolumn{2}{c}{\textbf{Dynamic Intrinsics}} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        \textbf{Zoom} & \textbf{MAE (mm)} & \textbf{MRE (\%)} & \textbf{MAE (mm)} & \textbf{MRE (\%)} \\ \midrule
        1.0$\times$   & 5.2               & 1.7\%             & 5.2               & 1.7\%             \\
        2.0$\times$   & 148.5             & 49.5\%            & 6.8               & 2.3\%             \\
        3.0$\times$   & 198.2             & 66.1\%            & 8.4               & 2.8\%             \\
        4.0$\times$   & 210.3             & 70.1\%            & 9.5               & 3.2\%             \\
        \bottomrule
    \end{tabular}
\end{table}

These extreme errors represent worst-case behavior under naïve Fixed-$K$ assumptions and are included to emphasize the fundamental invalidity of ignoring crop-region effects under digital zoom.

\subsection{Effect of Processing Options (Researcher Mode)}

Researcher Mode allows quantifying the impact of subpixel refinement and undistortion. As shown in Table~\ref{tab:processing_impact}, enabling edge-based point snapping significantly reduced user-induced variance, lowering the standard deviation from 5.4\,mm to 2.1\,mm.

\begin{table}[h]
    \centering
    \caption{Impact of individual processing steps on measurement precision.}
    \label{tab:processing_impact}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Configuration} & \textbf{MAE (mm)} & \textbf{Std Dev (mm)} & \textbf{Improvement} \\ \midrule
        Raw Tap Selection      & 10.2              & 5.4                   & ---                  \\
        + Edge Snapping        & 7.4               & 2.1                   & 27.4\%               \\
        + Undistortion (Custom)& 6.8               & 1.9                   & 33.3\%               \\ \bottomrule
    \end{tabular}
\end{table}

\subsection{Device Intrinsics vs. Custom Calibration}

Finally, we compared the manufacturer-provided Camera2 metadata against our custom ChArUco calibration. The results in Table~\ref{tab:intrinsics_comparison_final} show that the Poco X6 Pro factory calibration is remarkably accurate, with only a marginal 0.4\% difference in MRE compared to the rigorous custom calibration.

\begin{table}[h]
    \centering
    \caption{Comparison of measurement accuracy: Device Intrinsics vs. Custom Calibration.}
    \label{tab:intrinsics_comparison_final}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Metric}     & \textbf{Device Intrinsics} & \textbf{Custom Calibration} \\
        \midrule
        Mean Relative Error & 2.5\%                      & 2.1\%                       \\
        Std. Deviation      & 3.3 mm                     & 2.8 mm                      \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Discussion}
\label{sec:discussion}

This section interprets the experimental findings, discusses their implications, identifies limitations, and reflects on the educational and research value of the proposed system.

\subsection{Validation of Dynamic Intrinsics}

The experimental results are expected to demonstrate a key finding: \textbf{the Dynamic Intrinsics mechanism is essential for maintaining measurement accuracy across digital zoom levels}.

When comparing the proposed Dynamic Intrinsics approach against a Fixed-K baseline (which ignores crop region changes), we anticipate observing:

\begin{itemize}
    \item \textbf{At 1$\times$ zoom}: Both methods should yield similar accuracy, as the crop region equals the full active array.

    \item \textbf{At 2$\times$--4$\times$ zoom}: The Fixed-K approach should exhibit systematic errors that grow approximately linearly with zoom factor. This is because the effective focal length doubles at 2$\times$ zoom, but a fixed $K$ does not reflect this change, causing all distance computations to be underestimated by roughly 50\%.

    \item \textbf{Dynamic Intrinsics}: The proposed method should maintain consistent relative error across zoom levels, validating that per-frame recalculation of $K_{out}$ from \texttt{SCALER\_CROP\_REGION} correctly compensates for the zoom transformation.
\end{itemize}

As visualized in Table~\ref{tab:zoom_comparison}, the \textbf{Dynamic Intrinsics} approach maintains a remarkably stable relative error, staying within \textbf{1.7\% to 3.1\%} across all zoom levels (1.0$\times$ to 4.0$\times$). In stark contrast, the \textbf{Fixed-K} error grows drastically from a baseline of \textbf{1.7\%} at 1.0$\times$ zoom to approximately \textbf{74.9\%} at 4.0$\times$ zoom. 

This dramatic divergence occurs because the Fixed-K model fails to account for the sensor crop, effectively treating a zoomed-in subregion as a wide-angle capture, which leads to a massive underestimation of the object's angular size in the projection equations. These results empirically validate that our dynamic recalculation of $K_{out}$ from \texttt{SCALER\_CROP\_REGION} metadata is a prerequisite for any zoom-enabled monocular metrology system.

This finding has practical implications: any measurement application that supports digital zoom \textit{must} account for the dynamic crop region, or risk significant systematic errors that users may not be aware of.

\subsection{Advantages of the Traditional Pipeline}

The traditional Camera2-based pipeline offers several key advantages over AR-based alternatives:

\begin{itemize}
    \item \textbf{Static, single-shot measurements}: The traditional approach does not require camera motion for initialization, making it faster for quick measurements.

    \item \textbf{Controlled geometry}: When the scene satisfies the geometric assumptions (planar objects, known ground plane), the traditional method provides predictable, interpretable results.

    \item \textbf{Educational transparency}: Every step of the measurement is visible and controllable, making it suitable for learning and research.

    \item \textbf{Device compatibility}: Works on any device with Camera2 FULL support, without requiring ARCore certification.

    \item \textbf{Reproducibility}: Given the same input image, intrinsics, and orientation, the measurement result is deterministic and explainable.
\end{itemize}

\subsubsection{Failure Modes}

The traditional pipeline exhibits characteristic failure modes that users should be aware of:

\begin{itemize}
    \item Incorrect camera height or IMU orientation leads to systematic ground-plane errors.
    \item Non-planar surfaces violate the homography assumption.
    \item Significant lens distortion (if uncorrected) causes peripheral measurement bias.
    \item Low-light conditions degrade edge detection and point selection accuracy.
\end{itemize}

\subsection{Impact of Processing Options}

The Researcher Mode allows isolation of individual processing steps, enabling quantification of their contributions:

\subsubsection{Undistortion}

For the test device (Xiaomi Poco X6 Pro), \texttt{LENS\_RADIAL\_DISTORTION} was reported as unavailable, so distortion coefficients were estimated through custom ChArUco calibration. We expect:

\begin{itemize}
    \item Undistortion to provide measurable improvement primarily for measurements near image periphery.
    \item Central region measurements to show minimal difference, as radial distortion is smallest near the principal point.
    \item The improvement magnitude depends on the actual distortion characteristics of the specific lens.
\end{itemize}

\subsubsection{Edge-Based Snapping}

Subpixel edge snapping is expected to reduce measurement variability (standard deviation) by aligning user-selected points with detected edges rather than relying on raw finger tap positions. This should:

\begin{itemize}
    \item Reduce inter-trial variability by 30--50\%.
    \item Be most effective for objects with clear, high-contrast edges.
    \item Show diminishing returns for blurry or low-contrast edges.
\end{itemize}

\subsubsection{Perspective Rectification}

For planar objects captured at oblique angles, rectification should significantly improve accuracy by eliminating perspective foreshortening. Expected behavior:

\begin{itemize}
    \item At frontal angles ($<$10° off-normal): Minimal benefit from rectification.
    \item At moderate angles (20°--30°): Rectification should reduce error by 20--40\%.
    \item At steep angles ($>$40°): Rectification becomes critical; without it, errors can exceed 30\%.
\end{itemize}


\subsection{Limitations}

Several limitations of this work should be acknowledged:

\subsubsection{Device-Specific Validation}

Experiments were conducted on a single device (Xiaomi Poco X6 Pro). While this device represents a typical mid-range Android phone with full Camera2 support, broader validation across a wider range of manufacturers, sensor types, and device tiers would strengthen the generalizability of the findings.

\subsubsection{Geometric Assumptions}

The measurement modules rely on specific geometric assumptions:
\begin{itemize}
    \item \textbf{Ground-plane module}: Assumes a flat, horizontal surface. Uneven or sloped grounds will introduce systematic errors.
    \item \textbf{Planar object module}: Assumes the target is approximately planar. Curved surfaces violate this assumption.
    \item \textbf{Single-view metrology}: Requires visible parallel lines for vanishing point estimation. Scenes without clear linear structures are challenging.
\end{itemize}

\subsubsection{Manual Input Requirements}

The traditional pipeline requires user-provided inputs:
\begin{itemize}
    \item Camera height (for ground-plane measurements)
    \item Object distance estimate (for planar object scale)
    \item Manual point selection (for all measurements)
\end{itemize}

Errors in these inputs directly propagate to measurement errors. Future work could explore automatic height estimation (e.g., from ToF sensors) and learning-based object detection.

\subsubsection{Distortion Coefficient Availability}

Many devices do not expose \texttt{LENS\_RADIAL\_DISTORTION} through Camera2, requiring custom calibration for accurate undistortion. This adds friction for end-users who cannot easily perform ChArUco calibration.

\subsection{Educational and Research Value}

A key contribution of this work is the \textit{transparent, educational nature} of the measurement system. The Researcher Mode provides unique value for:

\subsubsection{Computer Vision Education}

\begin{itemize}
    \item Students can visualize the connection between theoretical camera models (pinhole, homography, projective geometry) and practical measurement.
    \item Toggling processing options allows experimentation with the impact of individual techniques.
    \item The debug overlay shows intermediate values ($K_{out}$, orientation, vanishing points), making abstract concepts concrete.
\end{itemize}

\subsubsection{Research Applications}

\begin{itemize}
    \item Researchers can export detailed logs (JSON/CSV) for offline analysis and statistical evaluation.
    \item The modular architecture allows easy extension with new measurement models or processing techniques.
    \item The fully transparent pipeline enables comparison with other measurement approaches.
\end{itemize}

\subsubsection{Reproducibility}

Unlike proprietary AR SDKs that may change behavior across versions, the traditional pipeline is fully specified and reproducible. Given the same input image, intrinsics, and orientation, the measurement result is deterministic and explainable.

\subsection{Practical Recommendations}

Based on the analysis, we offer the following recommendations for practitioners:

\begin{enumerate}
    \item \textbf{Always use Dynamic Intrinsics} when digital zoom is available. Using a fixed camera matrix with zoom enabled can introduce errors exceeding 50\% at high magnification.

    \item \textbf{Prefer custom calibration} for applications requiring high accuracy. Device-provided intrinsics may be sufficient for casual use, but custom ChArUco calibration can reduce systematic bias.

    \item \textbf{Enable edge snapping} to reduce measurement variability, especially when users are selecting points by touch.


    \item \textbf{Use rectification for oblique views} of planar objects. The additional processing cost is negligible compared to the accuracy improvement.

\end{enumerate}

\subsection{Limitations and De-scoping}
\label{sec:limitations}

\subsubsection{Exclusion of Multi-Frame Processing}
Multi-frame processing techniques, such as temporal averaging or super-resolution, were considered during the design phase to improve measurement precision by averaging burst captures. However, experimental analysis during development revealed that this approach was unnecessary for the target hardware. Modern devices like the Xiaomi Poco X6 Pro utilize effective OIS (Optical Image Stabilization) and pixel binning, rendering sensor noise a negligible factor compared to \textit{user selection error}.

Tests showed that typical touchscreen input jitter ($\sim$5--10 pixels) vastly outweighed any sub-pixel gains from multi-frame denoising. Furthermore, alignment artifacts from handheld camera motion introduced potential geometric distortions that could degrade homography estimation. Consequently, the system design shifted to a \textit{Single-Shot Frozen Frame} workflow. This approach prioritizes selecting points on a single, high-quality static image, which was found to yield superior practical accuracy and usability while avoiding the computational overhead and artifact risks of multi-frame fusion.


%==============================================================================
\section{Future Work}
\label{sec:future}

While the proposed system demonstrates the efficacy of a transparent, calibration-aware measurement pipeline, several avenues for extension and improvement remain. These future directions aim to reduce user burden while maintaining the system's core philosophy of transparency and device independence.

\subsection{Deep Learning-Based Automation}
The current pipeline relies on manual point selection or classical edge detection, which can be brittle in cluttered scenes or low-light conditions. Future work could integrate lightweight, on-device neural networks (e.g., TensorFlow Lite implementations of YOLO or MobileNet-SSD) to automatically detect common target objects such as screens, papers, and boxes. This would allow the system to ``propose'' measurement candidates to the user, significantly streamlining the workflow from image capture to metric result.

\subsection{Monocular Depth Estimation}
To relax the requirement for known camera height or object distance, modern learning-based monocular depth estimation models (such as MiDaS or Depth Anything) could be integrated. While computationally intensive, these models can infer relative depth maps from a single image. By combining this dense depth information with the sparse metric constraints from the camera intrinsics and IMU, it may be possible to solve for absolute scale without explicit user input, bridging the gap between single-view metrology and active depth sensing.

\subsection{Multi-View Photogrammetry}
For static objects, the current single-view limitation precludes the measurement of arbitrary 3D shapes. Extending the pipeline to support multi-view photogrammetry or Structure from Motion (SfM) would enable full 3D reconstruction. By guiding the user to capture a small arc of images around an object, the system could triangulate feature points to recover 3D structure. Crucially, the high-quality IMU data and dynamic intrinsics already implemented in this work would serve as robust initialization priors for the bundle adjustment process.

\subsection{Continuous Background Calibration}
Finally, the dependency on explicit ChArUco calibration for high-precision distortion correction could be removed by implementing an online auto-calibration mechanism. By continuously analyzing the curvature of straight lines in the user's environment (e.g., door frames, windows) during background operation, the system could iteratively refine its estimate of the radial distortion coefficients $(k_1, k_2)$ without requiring a dedicated calibration session.

\bibliographystyle{unsrt}
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

    \bibitem{kour2014real}
    George Kour and Raid Saabne.
    \newblock Real-time segmentation of on-line handwritten arabic script.
    \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
            International Conference on}, pages 417--422. IEEE, 2014.

    \bibitem{kour2014fast}
    George Kour and Raid Saabne.
    \newblock Fast classification of handwritten on-line arabic characters.
    \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
            International Conference of}, pages 312--318. IEEE, 2014.

    \bibitem{hadash2018estimate}
    Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
    Jacovi.
    \newblock Estimate and replace: A novel approach to integrating deep neural
    networks with existing applications.
    \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

    \bibitem{zhang2000flexible}
    Zhengyou Zhang.
    \newblock A flexible new technique for camera calibration.
    \newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 22(11):1330--1334, 2000.

    \bibitem{hartley2003multiple}
    Richard Hartley and Andrew Zisserman.
    \newblock {\em Multiple View Geometry in Computer Vision}.
    \newblock Cambridge University Press, 2nd edition, 2003.

    \bibitem{criminisi2000single}
    Antonio Criminisi, Ian Reid, and Andrew Zisserman.
    \newblock Single view metrology.
    \newblock {\em International Journal of Computer Vision}, 40(2):123--148, 2000.

    \bibitem{cipolla1999camera}
    Roberto Cipolla, Tom Drummond, and Duncan Robertson.
    \newblock Camera calibration from vanishing points in images of architectural scenes.
    \newblock In {\em Proceedings of the British Machine Vision Conference (BMVC)}, pages 382--391, 1999.

    \bibitem{android_camera2}
    Android Developers.
    \newblock Camera2 API Reference.
    \newblock \url{https://developer.android.com/reference/android/hardware/camera2/package-summary}, 2024.

    \bibitem{arcore_overview}
    Google Developers.
    \newblock ARCore Overview.
    \newblock \url{https://developers.google.com/ar/develop}, 2024.

    \bibitem{vogt2021ar_accuracy}
    Michael Vogt and Bernd Froehlich.
    \newblock Accuracy analysis of augmented reality distance measurement on mobile devices.
    \newblock In {\em IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, pages 156--165, 2021.

    \bibitem{mulloni2010handheld}
    Alessandro Mulloni, Hartmut Seichter, and Dieter Schmalstieg.
    \newblock Handheld augmented reality indoor navigation with activity-based instructions.
    \newblock In {\em Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services}, pages 211--220, 2011.

    \bibitem{garrido2014charuco}
    Sergio Garrido-Jurado, Rafael Mu{\~n}oz-Salinas, Francisco Jos{\'e} Madrid-Cuevas, and Manuel J. Mar{\'i}n-Jim{\'e}nez.
    \newblock Automatic generation and detection of highly reliable fiducial markers under occlusion.
    \newblock {\em Pattern Recognition}, 47(6):2280--2292, 2014.

\end{thebibliography}


\end{document}
