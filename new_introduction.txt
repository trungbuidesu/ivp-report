Over the past decade, mobile devices have witnessed a remarkable leap in optical hardware capabilities. Modern smartphones are equipped not only with high-resolution image sensors but also with optical image stabilization (OIS), advanced computational photography pipelines, and low-level hardware access through APIs such as Android's Camera2. This evolution has positioned smartphones as potential precision instruments for measurement tasks (metrology) across diverse domains including e-commerce, interior design, construction, and education.

Despite these hardware advances, realizing an accurate and mathematically transparent measurement tool on mobile platforms remains a significant challenge. Current software solutions tend toward two extremes: overly simplistic approaches that sacrifice accuracy for ease of use, or opaque ``black-box'' systems relying on complex augmented reality (AR) frameworks and proprietary SLAM (Simultaneous Localization and Mapping) algorithms. The former often yields measurement errors exceeding 10\%, while the latter, though potentially more accurate, obscures the underlying geometric principles and provides limited experimental control for researchers seeking to understand error sources or validate theoretical models.

This dichotomy creates a critical gap in the landscape of mobile computer vision applications. For educational purposes, research validation, and applications requiring verifiable accuracy, there exists a pressing need for a measurement system that combines the rigor of classical geometric computer vision with the accessibility of modern mobile platforms. Such a system must expose---rather than conceal---the mathematical foundations of single-view metrology, including the pinhole camera model, intrinsic parameter estimation, lens distortion correction, and plane-based measurement techniques.

A particularly underexplored challenge in mobile metrology is the dynamic nature of camera intrinsics during digital zoom operations. Unlike optical zoom systems with fixed focal lengths, digital zoom fundamentally alters the effective camera matrix through sensor cropping and image resampling. Existing measurement applications typically ignore these variations or rely on fixed calibration parameters, leading to systematic errors that compound with zoom level changes. Addressing this requires a rigorous geometric pipeline capable of recalculating intrinsic parameters in real-time based on the sensor's active region and crop metadata.

This work addresses these challenges by presenting a configurable monocular measurement system for Android that operates independently of third-party AR SDKs. The system implements what we term \textit{Dynamic Intrinsics}, a mechanism that continuously adjusts the camera matrix based on runtime metadata from the Camera2 API, specifically the \texttt{SCALER\_CROP\_REGION} and sensor characteristics. By fusing optical measurements with IMU-derived device orientation, the system supports traditional geometric measurement models including ground-plane homography for objects on horizontal surfaces, planar rectification for flat objects, and vanishing-point-based techniques for vertical structures.

Uniquely, the application features a dedicated \textit{Researcher Mode} designed to facilitate experimental inquiry. This mode enables granular control over processing variables such as radial distortion correction, calibration source selection (device-provided versus custom checkerboard calibration), edge-based point snapping, and multi-frame temporal averaging. All measurements are logged with full parameter provenance, allowing systematic comparison of different configurations and quantitative error analysis.

The contributions of this work are threefold: (1) a validated methodology for maintaining metric accuracy across continuous zoom levels through dynamic intrinsic recalibration, (2) an open experimental framework that exposes the complete measurement pipeline for educational and research purposes, and (3) empirical evidence demonstrating that classical computer vision techniques, when properly controlled for intrinsic parameter variations, can achieve competitive accuracy without the computational overhead and interpretability limitations inherent to deep learning or SLAM-based approaches.
